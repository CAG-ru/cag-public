{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Воспроизведение получения данных для исследования ЦПУР «Качество проведения оценки регулирующего воздействия в России: что показывает сплошной анализ текстовых данных?»\n",
    "\n",
    "Этот ноутбук – единая точка запуска всех скриптов, которые __собирают и обрабатывают данные__ с regulation.gov.ru и sozd.duma.gov.ru для дальнейшего использования в анализе.\n",
    "\n",
    "Ноутбук состоит из трех частей: __инициализации__, __получения датасета__, аналогичного датасету, [опубликованному на сайте ИНИД]('https://data-in.ru/data-catalog/datasets/177/'), и __нормализации__ заполнений граф сводных отчетов на отсутствующие, мусорные и содержательные.\n",
    "\n",
    "Получение датасета реализовано двумя способами: полной перегенерацией датасета, воспроизводящей работу аналитиков ЦПУР, и скачиванием датасета с сайта ИНИД.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Важно:</b> Сбор исходного датасета, размещённого на ИНИД, занимает <b>длительное время</b>: требуется обойти несколько тысяч страниц электронных порталов, а затем конвертировать, распознавать и парсить длинные документы. Поэтому рекомендуем пропустить эту часть и вместо неё переходить к части «Вариант 2: скачать датасеты с ИНИД»\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оглавление:\n",
    "* [Инициализация](#test)\n",
    "* [Вариант 1: скачать датасеты с ИНИД](#data-in-download)\n",
    "* [Вариант 2: полностью перегенерировать данные](#full-data-gen)\n",
    "* [Нормализация](#norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка зависимостей и инициализация <a class=\"anchor\" id=\"test\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'report_parser'...\n",
      "remote: Enumerating objects: 58, done.\u001b[K\n",
      "remote: Counting objects: 100% (58/58), done.\u001b[K(24/58)\u001b[K\n",
      "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
      "remote: Total 58 (delta 19), reused 34 (delta 6), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (58/58), 390.61 KiB | 3.99 MiB/s, done.\n",
      "Resolving deltas: 100% (19/19), done.\n"
     ]
    }
   ],
   "source": [
    "# Установим стандартные библиотеки через pip\n",
    "!pip install --quiet -r pip-requirements.txt\n",
    "\n",
    "# Установим библиотеку, разработанную ЦПУР для анализа отчётов\n",
    "!./install_report_parser.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим конфигурационный файл\n",
    "import json\n",
    "\n",
    "config = json.load(open('config.json', 'r'))\n",
    "workdir = config['working_directory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим в PYTHONPATH ссылки на вспомогательные скрипты\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "relative_lib_paths = ['utils/report_parser/',\n",
    "                      'utils/cleanup_utils/',\n",
    "                      'utils',\n",
    "                      'scraping',\n",
    "                      'gathering',\n",
    "                      'normalization'\n",
    "                     ]\n",
    "\n",
    "absolute_lib_paths = [os.path.abspath(x) for x in relative_lib_paths]\n",
    "for path in absolute_lib_paths:\n",
    "    sys.path.insert(0, path)\n",
    "\n",
    "from utils.report_parser.report import Report\n",
    "# Протестируем, что Report загрузился\n",
    "report_example = 'utils/report_parser/examples/report.html'\n",
    "Report(report_example)\n",
    "\n",
    "from utils.db_helper import DBHelper\n",
    "# Протестируем, что DBHelper загрузился\n",
    "_ = DBHelper(config['database'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим рабочую папку, если её ещё нет\n",
    "\n",
    "if not os.path.isdir(workdir):\n",
    "    os.mkdir(workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим другие полезные вещи\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариант 1: скачать датасеты с ИНИД <a class=\"anchor\" id=\"data-in-download\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>ToDo:</b> Заменить ссылку с Росгидромета на новый датасет после релиза\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скачаем и сложим датасеты в workdir/datasets\n",
    "\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "data_dir = os.path.join(workdir, 'datasets/')\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "url = '''\n",
    "    https://ds1.data-in.ru/Rosgidromet/Zagryaznenie_poverkhnostnih_vod_RF_176/Zagryaznenie_poverkhnostnih_vod_RF_176_23.09.21.zip\n",
    "'''\n",
    "with urllib.request.urlopen(url) as response, open(archive, 'wb') as out_file:\n",
    "    data = response.read() # a `bytes` object\n",
    "    out_file.write(data)\n",
    "    \n",
    "with zipfile.ZipFile(archive, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Вариант 2: полностью перегенерировать данные <a class=\"anchor\" id=\"full-data-gen\"></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Важно:</b> Если вы выполнили предыдущий пункт, этот проходить необзятельно.\n",
    "</div>\n",
    "\n",
    "[Перейти к следующей части](#norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скачаем данные проектов на ОРВ\n",
    "\n",
    "#### Данные с общей таблицы всех проектов\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>ToDo:</b> Нужно переименовать таблицы и колонки в базе, чтобы было как в датасете на ИНИД\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/partofspeech/Code/Repos/cpur/projects/orv_reports\n",
      "Начинается скачивание файла дампа.\n",
      "Queue submitted.\n",
      "column \"regulation_project_id\" of relation \"project\" does not exist\n",
      "LINE 1: INSERT INTO project (regulation_project_id, views_num, comme...\n",
      "                             ^\n",
      "\n",
      "120826 current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "120825 current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "120824 current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "120823 current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "120823 current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "120822 current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "120821 current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "120820 current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "120819 current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "120818 current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "120817 current transaction is aborted, commands ignored until end of transaction block\n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"scraping/regulation_parser.py\", line 404, in <module>\n",
      "    submit_tasks(get_arguments())\n",
      "  File \"scraping/regulation_parser.py\", line 383, in submit_tasks\n",
      "    scraper_executor.shutdown(wait=True)\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.8/concurrent/futures/thread.py\", line 236, in shutdown\n",
      "    t.join()\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.8/threading.py\", line 1011, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.8/threading.py\", line 1027, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!scraping/regulation_parser.py -c config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Файлы сводных отчетов и текстов НПА\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>ToDo:</b> Убрать лишние аргументы, типы документов \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>ToDo:</b> Переименования\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"./scraping/download_files.py\", line 252, in <module>\n",
      "    tasks, failed = make_queue(input_type, doc_type, degree)\n",
      "  File \"./scraping/download_files.py\", line 164, in make_queue\n",
      "    result = select_from_database(input_type, document_type, impact_degree)\n",
      "  File \"./scraping/download_files.py\", line 220, in select_from_database\n",
      "    return connection.select_statement(statement)\n",
      "  File \"/Users/partofspeech/Code/Repos/cpur/projects/orv_reports/utils/db_helper.py\", line 68, in select_statement\n",
      "    c.execute(statement)\n",
      "psycopg2.errors.UndefinedColumn: column \"number\" does not exist\n",
      "LINE 1: SELECT number, document_link FROM duma WHERE NOT document_li...\n",
      "               ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# тут надо переписать закачивание\n",
    "!./scraping/download_files.py -c config.json -i sozd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скачаем данные законопроектов, внесенных в Госдуму\n",
    "\n",
    "#### Сбор метаданных по АПИ\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>ToDo:</b> Запуск скрепера по АПИ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./duma_parser.py -c config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Файлы законопроектов\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>ToDo:</b> Запуск даунлоадера по регулейшен\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./download_files.py -c config.json -i duma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Соберём сводные отчёты из файлов\n",
    "\n",
    "На этом этапе мы с помощью библиотеки [report-parser]('https://github.com/CAG-ru/report_parser') преобразуем содержимое файлов, которые пока хранятся в формате отдельных файлов, в коллекцию питоновских объектов типа `Report` – то есть объектов, содержащих тексты и таблицы в формате `pandas.DataFrame`.\n",
    "\n",
    "Это долгий процесс, можете оставить скрипт работать, и пойти выпить самовар чаю – результаты на всякий случай сохранятся на диск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15096.docx File is not a zip file\n",
      "20367.pdf Расширение pdf не поддерживается\n"
     ]
    }
   ],
   "source": [
    "files_df = pd.DataFrame(columns=['regulation_project_id',\n",
    "                                'relative_path'])\n",
    "\n",
    "for degree in ['Низкая', 'Высокая', 'Средняя']:\n",
    "    files_dir = os.path.join(workdir, \n",
    "                            'main_ria_report') + '_' + degree   \n",
    "    pickled_path = os.path.join(workdir, \n",
    "                            'preparsed_reports') + '_' + degree\n",
    "    \n",
    "    reports = []\n",
    "    \n",
    "    for fn in os.listdir(files_dir):\n",
    "        report_path = os.path.join(files_dir, fn)\n",
    "        project_id = int(os.path.splitext(fn)[0])\n",
    "        files_df = files_df.append(pd.Series({\n",
    "            'regulation_project_id': project_id,\n",
    "            'relative_path' : report_path\n",
    "            }), ignore_index=True)\n",
    "        try:\n",
    "            report= Report(report_path)\n",
    "            report.parse()\n",
    "            reports.append((project_id, report))\n",
    "        except Exception as e:\n",
    "            print(fn, e)\n",
    "            \n",
    "    with open(pickled_path, 'wb') as f:\n",
    "        pickle.dump(reports, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Структурируем данные сводных отчетов\n",
    "\n",
    "На этом этапе проведём парсинг объектов типа Report в плоские таблицы: 1 основную и 15 вспомогательных.\n",
    "В основной таблице одной строчке соответствует сводный отчёт одного проекта, а колонки соответствуют графам отчёта, которые не предполагают множественных ответов.\n",
    "\n",
    "В случаях, когда графа отчёта может быть заполнена целым списком значений (например, «Цели предполагаемого регулирования»), для неё создаётся отдельная таблица с `one-to-many` соответствием: например, одному и тому же отчёту могут соответствовать несколько строк с разными целями регулирования.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>ToDo:</b> Поменять названия переменных, таблиц и тп\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заберем метаинформацию, она поможет нам с парсингом\n",
    "\n",
    "db = DBHelper(config['database'])\n",
    "query = '''\n",
    "    SELECT id, degreeregulatoryimpact_title\n",
    "    FROM public.project\n",
    "'''\n",
    "task = pd.DataFrame(db.select_statement(query),\n",
    "                   columns=['regulation_project_id', \n",
    "                            'impact_degree'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подцепим назад записанные на диск Report'ы\n",
    "\n",
    "reports = []\n",
    "for degree in ['Низкая', 'Высокая', 'Средняя']:\n",
    "    pickled_dir = os.path.join(workdir, \n",
    "                               'preparsed_reports') + '_' + degree \n",
    "    reports.extend(pickle.load(open(pickled_dir, 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим пустые датафреймы, куда будем складывать результаты\n",
    "\n",
    "with open(\"gathering/df_primers\", \"rb\") as f:\n",
    "    df_primers = pickle.load(f)\n",
    "main_df = df_primers[\"main_df\"]\n",
    "otm_tables = df_primers[\"otm_tables\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запустим парсеры, которые структурируют информацию из отчетов\n",
    "\n",
    "from gather_to_dataframe_utils import add_report_to_df, fill_blanks\n",
    "\n",
    "bad_reports = []\n",
    "for project_id, report in reports:\n",
    "    main_df = add_report_to_df(project_id, report, main_df, otm_tables, bad_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим лишние колонки – они возникают, когда кто-то случайно создает лишний заголовок в отчете\n",
    "main_df = main_df.iloc[:, :73]\n",
    "\n",
    "# Непустой 'header: id' – хороший предиктор адекватности отчета, \n",
    "# т.к. он заполняется автоматически и должен быть всегда\n",
    "main_df = main_df[~main_df['header: id'].isnull()]\n",
    "\n",
    "# Добавим нужные колонки и удалим ненужные\n",
    "main_df.drop(['header: id', 'header: regulation_project_id',\n",
    "              'header: start', 'header: end'], \n",
    "             inplace=True, axis=1)\n",
    "main_df = main_df.merge(task, on='regulation_project_id', how='inner')\n",
    "\n",
    "# Пометим специальным значением non-applicable графы, которые и не должны были быть заполнены\n",
    "main_df = main_df.apply(fill_blanks, axis=1)\n",
    "# Степень регулирующего воздействия больше не нужна\n",
    "main_df.drop(['impact_degree'], \n",
    "             inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>ToDo:</b> Стандартную форму не нужно искать по всем колонкам\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_standard_form(text):\n",
    "    if pd.isnull(text) or type(text) == int:\n",
    "        return text\n",
    "    text = text.replace('(указываются полное и краткое наименования)', '')\n",
    "    text = text.replace('(есть/нет)', '')\n",
    "    text = text.replace('(место для текстового описания)', '')\n",
    "    text = text.replace('(дней с момента принятия проекта нормативного правового акта)', '')\n",
    "    return text\n",
    "\n",
    "for column in main_df.columns:\n",
    "    main_df[column] = main_df[column].apply(remove_standard_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dir = os.path.join(workdir, 'datasets')\n",
    "\n",
    "if not os.path.isdir(parsed_dir):\n",
    "    os.mkdir(parsed_dir)\n",
    "\n",
    "main_df.to_csv(os.path.join(parsed_dir, 'ria_reports_main.csv'), \n",
    "             index=False)\n",
    "\n",
    "for otm_name, otm_df in otm_tables.items():\n",
    "    if otm_name == 'business':\n",
    "        otm_df[0].to_csv(os.path.join(parsed_dir,\n",
    "                'ria_reports_business_sizes_as_is.csv'), \n",
    "                index=False)\n",
    "        otm_df[1].to_csv(os.path.join(parsed_dir,\n",
    "                'ria_reports_business_profit_loss.csv'), \n",
    "                index=False)\n",
    "        otm_df[1].to_csv(os.path.join(parsed_dir,\n",
    "            'ria_reports_business_sizes_to_be.csv'), \n",
    "             index=False)\n",
    "        continue\n",
    "        \n",
    "    otm_df.to_csv(os.path.join(parsed_dir, 'ria_reports_' \\\n",
    "                               + otm_name + '.csv'), \n",
    "                               index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация <a class=\"anchor\" id=\"norm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>ToDo:</b> Описать, что здесь происходит\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ria_reports_business_profit_loss', 'ria_reports_business_sizes_as_is', 'ria_reports_business_sizes_to_be', 'ria_reports_cancel_duties', 'ria_reports_expenses', 'ria_reports_goals', 'ria_reports_group_changes', 'ria_reports_group_expenses', 'ria_reports_groups', 'ria_reports_kpi', 'ria_reports_main', 'ria_reports_necessary_measures', 'ria_reports_new_functions', 'ria_reports_notification_info', 'ria_reports_public_discussion', 'ria_reports_risks'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим датасеты\n",
    "\n",
    "data_dir = os.path.join(workdir, 'datasets/')\n",
    "main_df = pd.read_csv(data_dir + 'ria_reports_main.csv')\n",
    "\n",
    "otm_tables_fnames = sorted([fn for fn in os.listdir(data_dir) if not (fn.startswith('main') or fn.startswith('.'))])\n",
    "otm_tables = {fn[:-4]: pd.read_csv(data_dir + fn) for fn in otm_tables_fnames}\n",
    "otm_tables.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>ToDo:</b> Вынести clean_up и remove_standard_form в отдельный модуль в utils\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'is_junk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7fb9508e98fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m'-(место для текстового описания)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m ])\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_up\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-7fb9508e98fc>\u001b[0m in \u001b[0;36mclean_up\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_junk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'$#*!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_junk' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_up(text):\n",
    "    text = str(text)\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    if is_junk(text):\n",
    "        return '$#*!'\n",
    "        \n",
    "    text = text.rstrip('.')\n",
    "    text = text.rstrip(';')\n",
    "    text = text.replace('«', '\"').replace('»', '\"')\n",
    "    return text\n",
    "\n",
    "test = pd.Series([\n",
    "    '', '\\r', 'dfd\\n', '\\r\\n', '\\n\\r\\n', \n",
    "    'u', '.',\n",
    "    '-', '- --', \n",
    "    '____', \n",
    "    'nan', 'нет',\n",
    "    '(место для текстового описания)',\n",
    "    '-(место для текстового описания)'\n",
    "])\n",
    "test.apply(clean_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поиск самых частых значений\n",
    "def most_freq(series, num):\n",
    "    return sorted(Counter(series).items(), key=lambda item: (-item[1], item[0]))[:num]\n",
    "\n",
    "# Поиск самых коротких заполнений\n",
    "def shortest(series, num):\n",
    "    return sorted(Counter(series).items(), key=lambda item: (len(item[0]), item[1]))[:num]\n",
    "\n",
    "# Добываем топ самых коротких и частых значений\n",
    "def get_column_features(df, column, top_freq):\n",
    "    cleaned_field = df[column].apply(clean_up)\n",
    "    mf = pd.DataFrame(most_freq(cleaned_field, top_freq))\n",
    "    sh = pd.DataFrame(shortest(cleaned_field, top_freq))\n",
    "    return mf, sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разметка мусорных и отсутствующих заполнений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_fields' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ac021f23605e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mjunk_main_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_df_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'main'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mjunk_main_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'junk/junk_main_df.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-ac021f23605e>\u001b[0m in \u001b[0;36mfill_df_info\u001b[0;34m(dataframe, df_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mи\u001b[0m \u001b[0mвозвращает\u001b[0m \u001b[0mновый\u001b[0m \u001b[0mдатафрейм\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdf_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such table in dataset.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_fields' is not defined"
     ]
    }
   ],
   "source": [
    "with open('normalization/junk/junk_by_field.json', 'r') as fp:\n",
    "    junk_by_field = json.load(fp)\n",
    "        \n",
    "    \n",
    "def fill_df_info(dataframe, df_name):\n",
    "    \"\"\"\n",
    "    Функция принимает датафрейм, сравнивает значение нужных полей\n",
    "    с мусорными, проставляет соответствующую оценку\n",
    "    и возвращает новый датафрейм\n",
    "    \"\"\"\n",
    "    if df_name not in all_fields.keys():\n",
    "        print('No such table in dataset.')\n",
    "        return\n",
    "    \n",
    "    if 'header: id' in dataframe.columns:\n",
    "        id_column = 'header: id'\n",
    "    else:\n",
    "        id_column = 'id'\n",
    "    \n",
    "    result_df = pd.DataFrame()\n",
    "    for index, row in dataframe.iterrows():\n",
    "        result_df.loc[index, id_column] = row[id_column]\n",
    "        \n",
    "        for short, long in all_fields.get(df_name).items():    \n",
    "            value = dataframe.loc[index, long]\n",
    "            value = clean_up(value)\n",
    "\n",
    "            # Проверяем, есть ли текущее вхождение \n",
    "            # в \"мусоре\" для этого поля goals@timing\n",
    "            junk_set = junk_by_field.get(short)\n",
    "            if junk_set is not None and value in junk_set:\n",
    "                valid = 0\n",
    "            else:        \n",
    "                valid = 1\n",
    "\n",
    "            result_df.loc[index, short] = value\n",
    "            result_df.loc[index, str(short + '_valid')] = valid\n",
    "    return result_df\n",
    "\n",
    "junk_main_df = fill_df_info(main_df, 'main')\n",
    "junk_main_df.to_excel('junk/junk_main_df.xlsx', header=True)\n",
    "\n",
    "for _ in otm_tables.keys():\n",
    "    if _ in all_fields.keys():\n",
    "        df = otm_tables.get(_)\n",
    "        result = fill_df_info(df, _)\n",
    "        result.to_excel(f'junk/junk_{_}_df.xlsx', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
