{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9121e83a",
   "metadata": {},
   "source": [
    "# Поиск совпадающих текстов проектов НПА на `regulation.gov.ru` и на `sozd.duma.gov.ru`\n",
    "\n",
    "В этом ноутбуке показывается, как можно делать матчинг похожих терминологически текстов. Алгоритм, описанный здесь, использовался для поиска совпадений среди текстов законопроектов в докладе ЦПУР «Качество проведения оценки регулирующего воздействия в России: что показывает сплошной анализ текстовых данных?»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a9934",
   "metadata": {},
   "source": [
    "## Установка зависимостей и инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d8067",
   "metadata": {
    "cellId": "6h9ssavrsf33goofymvbtf"
   },
   "outputs": [],
   "source": [
    "# Пакеты для работы с текстами\n",
    "%pip install pymystem3\n",
    "%pip install pyaspeller\n",
    "from pymystem3 import Mystem\n",
    "from pyaspeller import YandexSpeller\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Для подсчета евклидова расстояния\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Для параллельной работы в таблицах\n",
    "import pandas as pd\n",
    "%pip install pandarallel\n",
    "import pandarallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795af68",
   "metadata": {},
   "source": [
    "## Функции для препроцессинга текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30819305",
   "metadata": {},
   "source": [
    "Наши тексты получены с помощью распознавания образов, поэтому на всякий случай можно попробовать удалить артефакты с помощью спеллера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "speller = YandexSpeller()\n",
    "def preprocess(text):\n",
    "    try:\n",
    "        corrected = speller.spelled(text)\n",
    "        return corrected\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b58c41",
   "metadata": {},
   "source": [
    "Следующий элемент обработки – разделение текста на токены, то есть самостоятельные кусочки: слова, знаки препинания, другие символы.\n",
    "Из-за того, что что у одного слова может быть много словоформ, а также потому, что в текстах НПА могут встречаться бесполезные короткие символы (слеши, переносы строк, № и другое), необходимо также выбросить мусор и провести лемматизацию – то есть отрезание флексии слова от неизменяемой основы. \n",
    "Всё это поможет сделать размерность итогового векторного пространства меньше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9f159",
   "metadata": {
    "cellId": "t7q1sh6lxgaiu0wf0t80ic"
   },
   "outputs": [],
   "source": [
    "stemmer = Mystem()\n",
    "\n",
    "def get_lemma(word, coerce_punkt=True):\n",
    "    if coerce_punkt:\n",
    "        blank = ''\n",
    "    else:\n",
    "        blank = word\n",
    "    stemmer_result = stemmer.analyze(word)\n",
    "    if not len(stemmer_result): \n",
    "        return blank\n",
    "    if 'analysis' not in stemmer_result[0]:\n",
    "        return blank\n",
    "    if not len(stemmer_result[0]['analysis']):\n",
    "        return blank\n",
    "    else:\n",
    "        return stemmer_result[0]['analysis'][0]['lex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c2f256",
   "metadata": {
    "cellId": "4qzpxd41bw8crrd3su9kyw"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = map(get_lemma, tokens)\n",
    "    return tuple(filter(lambda x: len(x), lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff2585",
   "metadata": {},
   "source": [
    "## Векторизуем нужные тексты\n",
    "\n",
    "Нам нужно векторизовать тексты, но не все: для `regulation.gov.ru` – только те, которые описывают __проекты законов__, а для `sozd.duma.gov.ru` – только те, которые были внесены не ранее начала введения ОРВ и были внесены Правительством (либо авторство неизвестно). Списки идентификаторов подходящих проектов собраны в таблицах `data/regulation_blanks.csv` и `data/duma_blanks.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde30e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_reg = pd.read_csv('data/regulation_blanks.csv', sep=';')\n",
    "task_sozd = pd.read_csv('data/duma_blanks.csv', sep=';')\n",
    "\n",
    "# Форматируем время\n",
    "\n",
    "task_reg['public_discussion_end'] = task_reg['public_discussion_end'].apply(pd.to_datetime, errors='coerce')\n",
    "task_sozd['introduction_date'] = task_sozd['introduction_date'].apply(pd.to_datetime, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скачаем из каталога ИНИД нужные датасеты \n",
    "\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "data_dir = 'data/'\n",
    "    \n",
    "url = '''\n",
    "https://ds1.data-in.ru/INSERT_LINK_HERE!'''\n",
    "with urllib.request.urlopen(url) as response, open(archive, 'wb') as out_file:\n",
    "    data = response.read() \n",
    "    out_file.write(data)  \n",
    "\n",
    "with zipfile.ZipFile(archive, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем тексты \n",
    "\n",
    "reg_texts = pd.read_csv('data/regulation_texts.csv', sep=';')\n",
    "sozd_texts = pd.read_csv('data/duma_texts.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cffe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выберем правильные тексты\n",
    "\n",
    "reg_texts = reg_texts[reg_texts.regulation_project_id.astype(str).isin(\n",
    "    task_reg.regulation_project_id.astype(str).to_list())]\n",
    "sozd_texts = sozd_texts[sozd_texts.duma_project_id.astype(str).isin(\n",
    "    task_sozd.duma_project_id.astype(str).to_list())]\n",
    "\n",
    "# Соберем в один корпус (так удобнее),\n",
    "# но сохраним количество текстов с regulation,\n",
    "# чтобы потом их разделить\n",
    "\n",
    "full_corpus = reg_texts.text.to_list() + sozd_texts.text.to_list()\n",
    "full_reg_num = len(reg_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизуем – это займёт долгое время!\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(2, 4), \n",
    "    preprocessor=preprocess, \n",
    "    tokenizer=tokenize, \n",
    "    min_df=2, \n",
    "    max_df=0.2)\n",
    "\n",
    "vectors = vectorizer.fit_transform(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим назад\n",
    "\n",
    "vectors_reg = [x for x in vectors[:full_reg_num]]\n",
    "vectors_sozd = [x for x in vectors[full_reg_num:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f3714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Хэшируем вектора, чтобы быстро их искать\n",
    "\n",
    "reg_text_lookup = dict(zip(reg_texts.filename.to_list(), vectors_reg))\n",
    "sozd_text_lookup = dict(zip(sozd_texts.filename.to_list(), \n",
    "                            [x.toarray() for x in vectors_sozd]))\n",
    "\n",
    "# Запишем в табличку\n",
    "task_reg['vector'] = task_reg.regulation_id.map(reg_text_lookup)\n",
    "\n",
    "# Выбросим наблюдения с неудачной векторизацией\n",
    "task_reg = task_reg[~task_reg.vector.isna()]\n",
    "task_sozd = task_sozd[task_sozd.gosduma_id.isin(sozd_text_lookup)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630399b",
   "metadata": {},
   "source": [
    "## Поиск ближайшего текста\n",
    "\n",
    "Это – самая трудоёмкая процедура, которая может занять несколько суток даже на большом количестве ядер.\n",
    "Чтобы облегчить задачу, вы можете сузить пространство поиска (например, приняв во внимание название или считая, что документ должен оказаться в Думе не позже, чем спустя год после того, как прошел ОРВ – у нас по умолчанию два года).\n",
    "\n",
    "Также при желании можно возвращать не только лучший матч, но и первые несколько лучших."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f1209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поиск ближайшего для наблюдения из регулейшена\n",
    "\n",
    "def find_best(row):\n",
    "    reg_id = row.regulation_id\n",
    "    reg_date = row.enddiscussion\n",
    "    reg_vector = row.vector.toarray()\n",
    "    \n",
    "    row['similarity'] = np.nan\n",
    "    \n",
    "    if not pd.isnull(reg_date):\n",
    "        possible_sozd = task_sozd[(task_sozd.gosduma_date >= reg_date) &\n",
    "                                 (task_sozd.gosduma_date.apply(lambda x: x.year - reg_date.year <= 2)) &\n",
    "                                  (task_sozd.gosduma_id.isin(sozd_text_lookup))].copy()\n",
    "    else:\n",
    "        possible_sozd = task_sozd.copy()\n",
    "        \n",
    "    possible_ids = possible_sozd.gosduma_id.to_list()\n",
    "    if len(possible_ids) < 1:\n",
    "        return row\n",
    "    \n",
    "    def calculate_similarity_by_id(gosduma_id):\n",
    "        sozd_vector = sozd_text_lookup[gosduma_id]\n",
    "        return float(cosine_similarity(reg_vector, sozd_vector))\n",
    "                \n",
    "    pandarallel.initialize()  \n",
    "    possible_sozd['similarity'] = possible_sozd.gosduma_id.parallel_apply(calculate_similarity_by_id)\n",
    "\n",
    "    if len(possible_sozd) > 0:\n",
    "        best = possible_sozd.sort_values('similarity', ascending=False).reset_index().loc[0]\n",
    "        row['gosuma_id'] = best.gosduma_id\n",
    "        row['similarity'] = best.similarity\n",
    "        row['gosduma_date'] = best.gosduma_date\n",
    "        row['gosduma_status'] = best.gosduma_status\n",
    "        row['gosduma_stage'] = best.gosduma_stage\n",
    "        row['gosduma_solution'] = best.gosduma_solution\n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4848a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем!\n",
    "best_matches = task_reg.apply(find_best, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bad156",
   "metadata": {},
   "source": [
    "## Поиск порогового значения\n",
    "\n",
    "Найдём пороговое значение, начиная с которого тексты будем считать совпадающими.\n",
    "Это необязательно `1.0` – формат текстов может отличаться, а также тексты могут серьёзно редактироваться.\n",
    "\n",
    "В докладе мы брали `0.75`-перцентиль похожести среди известных нам, то есть около `0.6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b4743b",
   "metadata": {
    "cellId": "0ey77bc2wtks01bo0v4x1twp"
   },
   "outputs": [],
   "source": [
    "# Известные матчи и не-матчи\n",
    "known_matches = pd.read_csv('data/known_matches.csv')\n",
    "\n",
    "# Интересные для теста id\n",
    "train_reg_ids = set(\n",
    "    known_matches.regulation_id.to_list())\n",
    "train_sozd_ids = set(\n",
    "    known_matches.gosduma_id.to_list())\n",
    "\n",
    "# Интересные для теста тексты\n",
    "reg_texts_train = reg_texts[reg_texts.filename.isin(train_reg_ids)].copy()\n",
    "sozd_texts_train = sozd_texts[sozd_texts.filename.isin(train_sozd_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6b959",
   "metadata": {
    "cellId": "ssgtj9a2csg3q8rw77hsh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Векторизуем только эти тексты\n",
    "corpus = reg_texts_train.text.to_list() + sozd_texts_train.text.to_list()\n",
    "reg_num = len(reg_texts_train.text.to_list())\n",
    "train_vectors = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94444d1a",
   "metadata": {
    "cellId": "z5adj8lep6zyerjeidrx"
   },
   "outputs": [],
   "source": [
    "# Разделим назад\n",
    "reg_texts_train['vectors'] = [x.toarray() for x in train_vectors[:reg_num]]\n",
    "sozd_texts_train['vectors'] = [x.toarray() for x in train_vectors[reg_num:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398eb4fd",
   "metadata": {
    "cellId": "bq5ip5qdevl3c1qbf68sn7"
   },
   "outputs": [],
   "source": [
    "# Подготовимся искать\n",
    "reg_lookup = dict(zip(reg_texts_train.filename.to_list(), reg_texts_train['vectors']))\n",
    "sozd_lookup = dict(zip(sozd_texts_train.filename.to_list(), sozd_texts_train['vectors']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8874e68",
   "metadata": {
    "cellId": "c0zrd4nf8pwcgeqcpzti2q"
   },
   "outputs": [],
   "source": [
    "# Подсчитаем расстояние для двух известных векторов\n",
    "import numpy as np\n",
    "\n",
    "def calculate_similarity(row):\n",
    "    reg_id = row.regulation_id\n",
    "    sozd_id = row.gosduma_id\n",
    "    try:\n",
    "        x = reg_lookup.get(reg_id)\n",
    "        y = sozd_lookup.get(sozd_id)\n",
    "        return float(cosine_similarity(x, y))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "known_matches['similarity'] = known_matches.apply(\n",
    "    calculate_similarity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313ad82",
   "metadata": {
    "cellId": "ussxj5xr9qql1643tz9c0i"
   },
   "outputs": [],
   "source": [
    "# Посмотрим описательные статистики\n",
    "known_matches[~known_matches.similarity.isna()].similarity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcb986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выберем пороговое значение\n",
    "threshold = 0.6\n",
    "\n",
    "# Соберём все достаточно совпадающие пары\n",
    "matches = best_matches[best_matches.similarity.apply(lambda x: not pd.isna(x) and x >= threshold)]\n",
    "matches.drop('vector', axis=1, inplace=True)\n",
    "matches.to_csv('data/matches.csv', sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "notebookId": "abb28093-69bd-4418-9e5c-63958c77373f"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
